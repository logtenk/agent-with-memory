services:
  llm_host:
    image: my/llmimg:v0             # reuse your CUDA-enabled dev image
    container_name: llm_host
    working_dir: /workspace
    # stdin_open: true
    # tty: true
    # init: true
    volumes:
      - ./:/workspace                    # your project code (hot-reload/dev)
      - /mnt/c/Users/icymi/.lmstudio/models:/models:ro  # models (ro is fine)
    environment:
      # Talk to services by Docker DNS name on internal network:
      LLAMACPP_BASE_URL: "http://llama:50000"
      CHROMA_PERSIST_ROOT: "./data/agents"
      HOST: "0.0.0.0"
      PORT: "50001"
      LD_LIBRARY_PATH: "/llama.cpp/build/bin:${LD_LIBRARY_PATH}"
      PYTHONPATH : "/workspace/src:${PYTHONPATH}"
    # Expose the wrapper to localhost:8888 only (not the whole host):
    ports:
      - "127.0.0.1:50001:50001"
    depends_on:
      - llama
      - mcp_duckduckgo
    command: >
      bash -lc "uvicorn agent_host.app.main:app --host 0.0.0.0 --port 50001 --reload"
    networks: [ llmnet ]
    # llm_host itself doesn't need GPU; remove if you prefer:
    # gpus: all

  # === llama.cpp server (internal-only) ===
  llama:
    image: my/llmimg:v0              # use your image that already has CUDA + built llama-server
    container_name: llama
    working_dir: /workspace
    volumes:
      - ./:/workspace
      - /mnt/c/Users/icymi/.lmstudio/models:/models:ro
      - ./cache:/cache                   # for slot/prompt-cache persistence
      - /mnt/c/proj/mcp/hostage/llama.cpp:/llama.cpp:ro  # llama.cpp source (for rebuilds)
    environment:
      LD_LIBRARY_PATH: "/llama.cpp/build/bin:${LD_LIBRARY_PATH}"
    # modern compose GPU flag:
    gpus: all
    # Internal-only service: use expose (no host published port)
    expose:
      - "8001"
    ports:
      - "127.0.0.1:50000:50000"
    command: >
      bash -lc "/llama.cpp/build/bin/llama-server
      --model /models/lmstudio-community/gemma-3-12b-it-GGUF/gemma-3-12b-it-Q4_K_M.gguf
      --mmproj /models/lmstudio-community/gemma-3-12b-it-GGUF/mmproj-model-f16.gguf
      --host 0.0.0.0 --port 50000
      --ctx-size 8192
      --n-gpu-layers 999
      --flash-attn on
      --slot-save-path /cache/slots
      --metrics"
    networks: [ llmnet ]

  # === MCP DuckDuckGo server ===
  mcp_duckduckgo:
    image: mcp/duckduckgo:latest
    container_name: mcp_duckduckgo
    expose:
      - "7801"                            # internal port
    ports:
      - "127.0.0.1:50002:50002"
    # Publish to localhost only for security:
    networks: [ llmnet ]

networks:
  llmnet:
    driver: bridge